{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/poojaanbu0/genai-lcel-expressions/blob/main/LCEL_Expressions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "044db1b7-c18c-45d2-be7a-29f027c901e2",
      "metadata": {
        "height": 62,
        "tags": [],
        "id": "044db1b7-c18c-45d2-be7a-29f027c901e2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "from dotenv import load_dotenv, find_dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd55c0a0-ca4e-4311-a33c-fcebeb7d8b1e",
      "metadata": {
        "height": 64,
        "id": "fd55c0a0-ca4e-4311-a33c-fcebeb7d8b1e"
      },
      "outputs": [],
      "source": [
        "# Load environment variables (OpenAI API Key)\n",
        "_ = load_dotenv(find_dotenv())\n",
        "openai.api_key = os.environ['OPENAI_API_KEY']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d036bb8e-8ca7-4dbd-8103-f50a3c8c3af9",
      "metadata": {
        "height": 111,
        "id": "d036bb8e-8ca7-4dbd-8103-f50a3c8c3af9"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import DocArrayInMemorySearch\n",
        "from langchain.schema.runnable import RunnableMap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8955cff7-f1a2-4f94-ab5b-fcdda0859702",
      "metadata": {
        "height": 164,
        "id": "8955cff7-f1a2-4f94-ab5b-fcdda0859702"
      },
      "outputs": [],
      "source": [
        "# Step 1: Vector Store Creation\n",
        "# Create a vector store from given texts\n",
        "vectorstore = DocArrayInMemorySearch.from_texts(\n",
        "    [\"harrison worked at kensho\", \"bears like to eat honey\"],\n",
        "    embedding=OpenAIEmbeddings()\n",
        ")\n",
        "\n",
        "# Convert the vector store into a retriever\n",
        "retriever = vectorstore.as_retriever()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2df87934-1697-405c-b460-5e9bfd16c792",
      "metadata": {
        "height": 134,
        "id": "2df87934-1697-405c-b460-5e9bfd16c792"
      },
      "outputs": [],
      "source": [
        "# Step 2: Prompt Template\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "871cb26b-97b3-4f63-8bb3-523d3e6d117b",
      "metadata": {
        "height": 62,
        "id": "871cb26b-97b3-4f63-8bb3-523d3e6d117b"
      },
      "outputs": [],
      "source": [
        "# Step 3: Model Definition\n",
        "# Define the OpenAI chat model\n",
        "model = ChatOpenAI(model_name=\"gpt-3.5-turbo\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "127a7fb6-5821-4934-ab56-9e3300516c05",
      "metadata": {
        "height": 48,
        "id": "127a7fb6-5821-4934-ab56-9e3300516c05"
      },
      "outputs": [],
      "source": [
        "# Define an output parser to format the output\n",
        "output_parser = StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ec01c56-731c-4e4f-a5f6-493fba953db0",
      "metadata": {
        "height": 116,
        "id": "4ec01c56-731c-4e4f-a5f6-493fba953db0"
      },
      "outputs": [],
      "source": [
        "# Step 4: LCEL Chain Definition\n",
        "# Define a RunnableMap that processes input to extract context, format the prompt, and generate an answer\n",
        "chain = RunnableMap({\n",
        "    \"context\": lambda x: retriever.get_relevant_documents(x[\"question\"]),\n",
        "    \"question\": lambda x: x[\"question\"]\n",
        "}) | prompt | model | output_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9ca6506-826f-4420-8f19-25dd4dbbc1dc",
      "metadata": {
        "height": 80,
        "id": "a9ca6506-826f-4420-8f19-25dd4dbbc1dc",
        "outputId": "7c1ec500-4804-4986-b77c-91e8e06ff6fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response 1: Harrison worked at Kensho.\n",
            "Response 2: Bears like to eat honey.\n"
          ]
        }
      ],
      "source": [
        "# Step 5: Testing the Chain\n",
        "# Example questions for testing\n",
        "question_1 = {\"question\": \"where did harrison work?\"}\n",
        "question_2 = {\"question\": \"what do bears like to eat?\"}\n",
        "\n",
        "# Invoke the chain for each question\n",
        "response_1 = chain.invoke(question_1)\n",
        "response_2 = chain.invoke(question_2)\n",
        "\n",
        "# Print the outputs\n",
        "print(f\"Response 1: {response_1}\")\n",
        "print(f\"Response 2: {response_2}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "707d1319-8840-4ed5-b4a4-a2a128799db6",
      "metadata": {
        "height": 73,
        "id": "707d1319-8840-4ed5-b4a4-a2a128799db6",
        "outputId": "0dd87b94-9273-422d-d8a6-cc02fb6c3e0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response from inputs.invoke: {'context': [Document(page_content='harrison worked at kensho'), Document(page_content='bears like to eat honey')], 'question': 'where did harrison work?'}\n"
          ]
        }
      ],
      "source": [
        "# Step 6: Inputs Handling (Optional, Additional Example)\n",
        "inputs = RunnableMap({\n",
        "    \"context\": lambda x: retriever.get_relevant_documents(x[\"question\"]),\n",
        "    \"question\": lambda x: x[\"question\"]\n",
        "})\n",
        "\n",
        "# Test with inputs.invoke\n",
        "response_from_inputs = inputs.invoke({\"question\": \"where did harrison work?\"})\n",
        "print(f\"Response from inputs.invoke: {response_from_inputs}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}